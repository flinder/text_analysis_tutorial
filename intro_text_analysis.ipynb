{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Computational Text Analysis - The Boring Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introduction\n",
    "\n",
    "###### What, Why, for Who?\n",
    "This is a little tutorial I gave for my visualization class in the Geography Department at Penn State. I introduce the very basic steps that are necessary to get messy text collected online or in other places into a form to be usable for more sophisticated text analysis tools. There are a lot of great resources out there that explain all kinds of cool things that you can do with text, but there are a lot of nitty gritty details to consider before you can get started. As an example I use some twitter data I collected with the search term `Trump`.  \n",
    "\n",
    "There are a few tutorials for the basics of text analysis (e.g. see [here](), [here]() and [here]()), but they mostly are focused on R and use packages or modules that obscure what actually is happening under the hood. I prefer to demonstrate with basic (slow and not optimized) code what is happening; most of the steps are not very compicated anyway. \n",
    "\n",
    "This tutorial is aimed at beginners in text analysis with little experience with computational tools. I provide actual example code that demonstrates how every idea would be realized. Everything is written in basic python. For those who are not familiar with programming I try to explain every step so you don't have to know python in order to follow. \n",
    "\n",
    "You can find the whole tutorial as an ipython notebook on my [github page](INSERT LINK).\n",
    "\n",
    "\n",
    "###### Cool examples of text analysis\n",
    "Here are some examples of interesting scientific work that has been done using computational text analysis. \n",
    "\n",
    "[Put some links here]\n",
    "\n",
    "\n",
    "\n",
    "###### Roadmap\n",
    "\n",
    "To get from text that you find in the real world, to something you can work with requires many steps. Which ones you take will also depend heavily on the task you want to acomplish in the end. It is therefore hard to give a general tutorial that will work in every scenario. But many steps are required in every application so I will discuss the most important ones.\n",
    "\n",
    "In this tutorial I explain how to get from a text corpus (collection of documents) to a (usable) document term matrix (DTM). A lot (but not all) statistical analyses of text are based on DTMs. The core idea is to assume that all the information in a document is contained in the frequency of each word. Then each document is represented as a vector of length `v`, where `v` is the number of unique words in the whole corpus. Each element of the vector gives a count (or other measure) of how often the word appears in the document. The DTM is then a matrix where each row is a document vector. \n",
    "\n",
    "Here's an example. Let's look three documents:\n",
    "\n",
    "    1. my dog likes my porcupine\n",
    "    2. my porcupine doesn't like my dog\n",
    "    3. my dog eats my porcupine\n",
    "\n",
    "The vocabulary is `V = [\"my\", \"dog\", \"likes\", \"porcupine\", \"doesn't\", \"like\", \"eats\"]`. So the DTM is:\n",
    "\n",
    "```\n",
    "    My dog likes porcupine doesn't like eats\n",
    "1.  2  1   1     1         0       0    0\n",
    "2.  2  1   0     1         1       1    0\n",
    "3.  2  1   0     1         0       0    1\n",
    "```\n",
    "\n",
    "Note that this matrix can also be interpreted geoetrically, where each row-vector describes the location of its corresponding document in a space with `v` dimensions. This fact is very important for more sophisticated text analysis techniques. For example, similarity of documents is often measured as the cosine of the angle between two document vectors. \n",
    "\n",
    "In order to get to this matrix and especially to get to a usable matrix that is of manageable size, there are several steps to take. I will demonstrate the following things:\n",
    "\n",
    "- Loading Text, Encodings, Memory\n",
    "- Manipulating Text: Regular Expressions\n",
    "- Tokenization\n",
    "- Generating the DTM\n",
    "- The Sparsity Problem\n",
    "- Text Preprocessing\n",
    "- Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Text, Encodings, Memory\n",
    "\n",
    "For the rest of the tutorial we will be working with twitter data. I collected about 2000 tweets from the [Twitter Streaming API](https://dev.twitter.com/streaming/overview) ([This](http://adilmoujahid.com/posts/2014/07/twitter-analytics/) is a good tutorial on how to collect Twitter data with Python) using the search term `Trump`.\n",
    "\n",
    "This dataset is very small an it is normally not a problem to just load it into memory. However, especially when working with social media data, size can quickly be a problem. Since this is a basic intro I will not go into detail, but I will show the difference between loading all your data into memory and then working on it and processing only small portions of data at a time (streaming the data). I think it is good practice to do this whenever possible.\n",
    "\n",
    "Since we will be working with twitter data for the class project, I will make a little detour and show how to load one tweet at a time from disk, extract the text of the tweet and discard all other information. This way we can process large quantities of data with less memory.\n",
    "\n",
    "Let's take a look at the data first to make this clearer. The first tweet we collected looks like this:\n",
    "\n",
    "```\n",
    "{\"created_at\":\"Tue Mar 15 18:54:26 +0000 2016\",\"id\":709815032113184768,\n",
    " \"id_str\":\"709815032113184768\",\n",
    " \"text\":\"Ben Carson Reveals the Real Reasons He Endorsed Donald Trump. #WhatWouldYouDoForAKlondikeBar?\",\n",
    " \"source\":\"\\u003ca href=\\\"http:\\/\\/twitter.com\\\" rel=\\\"nofollow\\\"\\u003eTwitter Web Client\\u003c\\/a\\u003e\",\n",
    " \"truncated\":false,\n",
    " \"in_reply_to_status_id\":null,\n",
    " \"in_reply_to_screen_name\":null,\n",
    " \"user\":{\"id\":2937476451,\n",
    " \"id_str\":\"2937476451\",\n",
    " \"name\":\"Nate Madden\",\n",
    " \"screen_name\":\"NateMadden_IV\",\n",
    " \"location\":\"Washington, DC\",\n",
    "  ...\n",
    "  }\n",
    "```\n",
    "\n",
    "The data is in json format. Each json document is surrounded by `{}` and has keys (`\"created_at\", \"id_str\", \"text\",...`) and values (`\"Tue Mar 15...\", \"7098...\", \"Ben Carson Reveals...\"`). I don't display all the information here, but each tweet has at about 100 such key - value pairs. \n",
    "\n",
    "For this tutorial we only need the value of the `\"text\"` key, which contains the text the user wrote in her tweet. We will therefore load only one tweet at a time, extract the text and discard all the other information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving information from tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io \n",
    "\n",
    "tweet_file_connection = io.open('data.json', mode='r', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This line opens a connection to the file on disk the file contains one tweet per line.  Note that this is just a pipe to the data in the file, the object 'tweet_file_connection' does not contain any tweets yet. In order to get at the tweets, we can *iterate* over the lines of the file. Let's extract the text of the tweet and store it in memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json # Module to make python understand the json data structure\n",
    "\n",
    "# Empty list to strore the tweets\n",
    "tweets = []\n",
    "\n",
    "# Loop through the file, one line at a time\n",
    "for line in tweet_file_connection:\n",
    "\n",
    "    # Parse the data structure, so we can access the keys directly\n",
    "    tweet = json.loads(line)\n",
    "    \n",
    "    # Retrieve the text and author id\n",
    "    text = tweet['text']\n",
    "        \n",
    "    # Store it in the list\n",
    "    tweets.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1521\n",
      "[u'RT @donnabrazile: Amazing. https://t.co/SfxHD92gCJ', u'@Miami4Trump @RogerJStoneJr @jwlancasterjr @CLewandowski_ The ploy is to leave off Trump name &amp; tell people to vote Wed w/corrected ballots']\n"
     ]
    }
   ],
   "source": [
    "print len(tweets)\n",
    "print tweets[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It did we collected 1521 tweets and below we can see the text of the first two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Text: Regular Expressions\n",
    "\n",
    "Regular expressions are basically templates that can be used to match character sequences. You have probably used them. For example when you type a search into a search engine you can use the `*` to match several terms with one query. For example `read*` would match `reading`, `reader`, `readings`, `reads`, etc. In this case `*` is a regular expression that says 'match every character except a space'. \n",
    "\n",
    "Regular expressions differ slightly between programming languages. This [regex tester](https://regex101.com/#python) is a really cool ressource to test your regular expressions. \n",
    "\n",
    "In python regular expressions are contained in the `re` module. We will first write some regular expressions to do some simple things and then do something slightly more complex. \n",
    "\n",
    "Let's start simple. Consider the following sentence:\n",
    "\n",
    "    \"It's just a movie Donald\"\n",
    "    \n",
    "We will first write a regular expression that replaces all vowels with `_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's j_st _ m_v__ D_n_ld\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the re module\n",
    "import re\n",
    "\n",
    "s = \"It's just a movie Donald\"\n",
    "\n",
    "# This is the regular expression. The brakets are a special character that says \n",
    "# \"match each character in this set of characters\"\n",
    "vowels = '[aeiou]'\n",
    "\n",
    "# The sub() funciton in the re module allows us to substitute stuff for the \n",
    "# regular expression\n",
    "re.sub(pattern=vowels, repl=\"_\", string=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This worked well. We saw that the `[` `]` where key to do this. There are a lot of these special characters, which you just have to look up. Here are some of the most important basics:\n",
    "\n",
    "### Some basic expressions\n",
    "\n",
    "- `[]`: A set of characters/expressions. E.g. `[A-Z]` matches all upper case letters\n",
    "- `.`: Matches everything\n",
    "- `+`: Matches one or more of the preceding character\n",
    "- `?`: Matches 0 or one repetitions of the preceding character\n",
    "- `\\n`: Newline\n",
    "- `\\t`: Tab\n",
    "- `\\s`: Every space character, e.g. `\\n`, `\\t`, `\\r`\n",
    "- `^`: Beginning of string\n",
    "- `$`: End of string\n",
    "- `[^]`: Negation of a set, e.g. `[^A-Z]` matches everything that's not an uppercase letter\n",
    "\n",
    "Just some examples. See the [documentation]() for an exhaustive list and use Google and [regex tester](https://regex101.com/#python) extensively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more simple example. Let's find all the upper case words in the example sentence and replace them with `MATCH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MATCH just a movie MATCH'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The regular expression. Again the brackets for a group. [A-Z] matches all uppercase letters from A-Z. \\S matches\n",
    "# everything BUT space characters and + means repeat the previous character until you encounter one that doesn't fit.\n",
    "# So in words: \"Match everything that starts with an upper case letter, followed by arbitrarily many other characters\n",
    "# until you hit a space\n",
    "upper_case_word = \"[A-Z]\\S+\"\n",
    "re.sub(pattern=upper_case_word, repl='MATCH', string=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess you can see that regular expressions can be very helpful in many different situations. Let's use it with our example. Maybe we want to find all hashtags (`#textAnalysis`) and twitter handles (`@frido`), count the and find the most common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our regular expressions:\n",
    "\n",
    "# Everything that starts with `#` followd by arbitrarily many repetitions of everything but (^ is the negation of the \n",
    "# gorup in the brackets) `space`, `end of string`, `newline`. And the same thing for handles with @.\n",
    "# We can compile the regular expression, because we use \n",
    "re_hash = '#[^\\s$\\n]+'\n",
    "re_hand = '@[^\\s$\\n]+'\n",
    "\n",
    "# Loop throuh all the tweets and find all matches with the findall() method. Findall returns a list \n",
    "# of all matches\n",
    "\n",
    "# Empty dictionaries to store each handle/hastag and a count of how often it appears\n",
    "handles = {}\n",
    "hashtags = {}\n",
    "\n",
    "for tweet in tweets:\n",
    "    hand = re.findall(pattern=re_hash, string=tweet)\n",
    "    hasht = re.findall(pattern=re_hand, string=tweet)\n",
    "    \n",
    "    # For each handle we found, check if we saw it already\n",
    "    # (if it's in our dictionary), if so increment the count\n",
    "    # by one, if not make a new entry and set the count to 1\n",
    "    \n",
    "    for handle in hand:\n",
    "        if handle in handles:\n",
    "            handles[handle] += 1\n",
    "        else:\n",
    "            handles[handle] = 1\n",
    "            \n",
    "    # Same thing for the hashtags        \n",
    "    for hashtag in hasht:\n",
    "        if hashtag in hashtags:\n",
    "            hashtags[hashtag] += 1\n",
    "        else:\n",
    "            hashtags[hashtag] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#GOPConvention: 1\n",
      "#Kasich4Us: 1\n",
      "#VR's: 1\n",
      "#unitewithcruz: 1\n",
      "#BenCarson:: 1\n",
      "#Endorse: 1\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 entries of handles:\n",
    "for index, handle in enumerate(handles):\n",
    "    print handle + \": \" + str(handles[handle])\n",
    "    if index == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Most hashtags seem to appear only once in this small collection of tweets. Let's see what the most common hashtags and handles are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Trump: 64\n",
      "@realDonaldTrump: 75\n"
     ]
    }
   ],
   "source": [
    "max_hand = max(handles, key=handles.get)\n",
    "max_hash = max(hashtags, key=hashtags.get)\n",
    "\n",
    "print max_hand + \": \" + str(handles[max_hand])\n",
    "print max_hash + \": \" + str(hashtags[max_hash])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Most statistical text analysis is based on the 'bag-of-words' approach: It is assumed (clearly wrongly) information in documents is purely contained in the word counts of a document. Grammatical and syntactical structure is ignored. Let's first count all words in each document.\n",
    "\n",
    "To do this we have to split the document into discrete words, they can are also called 'tokens' and the process 'tokenization'. Here we simply split the string object, whenever there is a white space (you might already think of things that can go wrong here). There are more sophisticated methods to do this but for now it is sufficient. \n",
    " \n",
    "After obtaining tokens each document is represented as the count of each unique token in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out for one tweet first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@faineg @JM_Ashby NEVER treat your opponents with humanity, that's just weakness and ISIS and/or Trump only respects STRENGTHs\n"
     ]
    }
   ],
   "source": [
    "example = tweets[4]\n",
    "print example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'@faineg', u'@JM_Ashby', u'NEVER', u'treat', u'your', u'opponents', u'with', u'humanity,', u\"that's\", u'just', u'weakness', u'and', u'ISIS', u'and/or', u'Trump', u'only', u'respects', u'STRENGTHs']\n"
     ]
    }
   ],
   "source": [
    "tokens = example.split(' ')\n",
    "print tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This works. Now we just have to count how often each token appears per tweet and do it for all tweets. After splitting them we will count how often every word appears and store it in a data structure called `dictionary`. A dictionary works like `json`, that is each entry of the dictionary consists of a key and a value. In our case its `'token': count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Empty list to store the 'bag-of-words' representation of each tweet\n",
    "bow_tweets = []\n",
    "\n",
    "# Loop through all tweets\n",
    "for tweet in tweets:\n",
    "    \n",
    "    tokens = tweet.split(' ')\n",
    "    \n",
    "    # The tweet will be stored as a dictionary with entries 'token': count\n",
    "    counts = {}\n",
    "    \n",
    "    for token in tokens:\n",
    "        \n",
    "        # New entry if token is not yet in dictionary\n",
    "        # If it's there already, increase the count\n",
    "        if token in counts:\n",
    "            counts[token] += 1        \n",
    "        else:\n",
    "            counts[token] = 1\n",
    "    \n",
    "    # Store it in the list\n",
    "    bow_tweets.append(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what it looks like for the first three tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'RT': 1, u'@donnabrazile:': 1, u'https://t.co/SfxHD92gCJ': 1, u'Amazing.': 1}, {u'leave': 1, u'@CLewandowski_': 1, u'off': 1, u'@Miami4Trump': 1, u'people': 1, u'&amp;': 1, u'is': 1, u'Wed': 1, u'ploy': 1, u'name': 1, u'@jwlancasterjr': 1, u'to': 2, u'w/corrected': 1, u'tell': 1, u'vote': 1, u'The': 1, u'ballots': 1, u'@RogerJStoneJr': 1, u'Trump': 1}, {u'RT': 1, u'@Basseyworld:': 1, u'win,': 1, u'Trump': 1, u\"don't\": 1, u'I': 1, u'together': 1, u'is': 1, u'get': 1, u'it': 1, u'whatever': 1, u'to': 1, u'so': 1, u'a': 1, u'want': 1, u'Dems.': 1, u'hell': 1, u'the': 1, u'Republican': 1, u'or': 1}]\n"
     ]
    }
   ],
   "source": [
    "print bow_tweets[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "More sophisticated tokenizers can detect things like it's -> it is and deal correctly with tokenization. They are also able to separate sentences. For python there is the classic [nltk](http://www.nltk.org/) module ([here](http://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize) is a tutorial on tokenization) and the faster [spaCy](https://spacy.io/). The [scikit-learn](http://scikit-learn.org/stable/) machine learning libraries also have tokenization tools, the [sklearn vectorizers](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). In R you can use [tm](https://cran.r-project.org/web/packages/tm/index.html) or [quanteda](https://cran.rstudio.com/web/packages/quanteda/vignettes/quickstart.html).\n",
    "\n",
    "For simple tasks like this, there are also some really nice UNIX commandline tools. For exampl: [tr](https://en.wikipedia.org/wiki/Tr_(Unix) and [sed](http://www.grymoire.com/Unix/Sed.html). Check out [UNIX for poets](https://web.stanford.edu/class/cs124/kwc-unix-for-poets.pdf) too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Document Term Matrix\n",
    "\n",
    "A lot (but not all) statistical analyses of text are based on Document Term Matrices (DTM). A DTM, as the name says, is a matrix that contains one row per document (in our case a document is all the text for one candidate) and one column per term (or token). Like this:\n",
    "```\n",
    "        token_1 token_2 token_3 ...\n",
    "tweet_1 4       3       0       ...\n",
    "tweet_2 0       2       0       ...\n",
    "tweet_3 3       1       1       ...\n",
    "...     ...     ...     ...     ...\n",
    "```\n",
    "\n",
    "That means that each document is represented as a vector in a space that has as many dimensions as there are unique terms in the collection of all documents (the vocabulary of the corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the vocabulary we first have to know all the unique tokens in the our collection of tweets. Then we can generate the vector for each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7739\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary\n",
    "\n",
    "# Empty set to store the vocabulary in (sets have a much faster lookup time \n",
    "# compared to lists, read about hashmaps if you want to know more, it's genious!)\n",
    "\n",
    "vocabulary = set()\n",
    "for tweet in bow_tweets:\n",
    "    vocabulary.update(tweet.keys())\n",
    "    \n",
    "print len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 7739 unique words in our 1500 tweets. That sounds like a lot. Let's look at a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'',\n",
       " u'#BenCarson:',\n",
       " u'four',\n",
       " u'Does',\n",
       " u'@peddoc63',\n",
       " u'dogshit,',\n",
       " u'Cruz.....for',\n",
       " u'@UTHornsRawk',\n",
       " u'candidates.',\n",
       " u'https://t.co/nth63AgX5C']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocabulary)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the first few entries there is probably a relatively large amount of words that appear only once. We probalby also don't want `candidate.` and `candidate` to be two separate words... But I'm getting ahead of myself.\n",
    "\n",
    "Let's make our first term document matrix first and then consider the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1521, 7739)\n"
     ]
    }
   ],
   "source": [
    "# Generate the vectors and store them in a Matrix\n",
    "import pandas as pd # Module for dataframes\n",
    "\n",
    "# Create an empty dataframe to hold the data\n",
    "# Note that this is a very inefficient way to do this. In practice \n",
    "# you wouldn't construct a data frame like this but it's good for \n",
    "# explaining the concept\n",
    "dtm = pd.DataFrame(columns = list(vocabulary))\n",
    "\n",
    "i = 0\n",
    "for tweet in bow_tweets:\n",
    "       \n",
    "    vector = []\n",
    "    # For each word in the vocabulary check if it is in the tweet\n",
    "    # if yes, append the count obtained befoe if not append a 0\n",
    "    for token in dtm.columns:\n",
    "        if token in tweet:\n",
    "            vector.append(tweet[token])\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    \n",
    "    # Append the vector to the matrix as row i\n",
    "    dtm.loc[i] = vector\n",
    "    \n",
    "    # Increase i by one\n",
    "    i += 1\n",
    "\n",
    "# Print the dimensions of the matrix\n",
    "print dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a matrix of shape 1521 (number of tweets/documents) x 7739 (number of unique terms). Let's see what this matrix looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>#BenCarson:</th>\n",
       "      <th>four</th>\n",
       "      <th>Does</th>\n",
       "      <th>@peddoc63</th>\n",
       "      <th>dogshit,</th>\n",
       "      <th>Cruz.....for</th>\n",
       "      <th>@UTHornsRawk</th>\n",
       "      <th>candidates.</th>\n",
       "      <th>https://t.co/nth63AgX5C</th>\n",
       "      <th>...</th>\n",
       "      <th>“Democrats</th>\n",
       "      <th>kinda</th>\n",
       "      <th>musulmanes,</th>\n",
       "      <th>https://t.co/CTBIkN50VE</th>\n",
       "      <th>coffee.</th>\n",
       "      <th>Garbage</th>\n",
       "      <th>metà,</th>\n",
       "      <th>Contributor</th>\n",
       "      <th>understand</th>\n",
       "      <th>Adams:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7739 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      #BenCarson:  four  Does  @peddoc63  dogshit,  Cruz.....for  \\\n",
       "0  0            0     0     0          0         0             0   \n",
       "1  0            0     0     0          0         0             0   \n",
       "2  0            0     0     0          0         0             0   \n",
       "3  0            0     0     0          0         0             0   \n",
       "4  0            0     0     0          0         0             0   \n",
       "\n",
       "   @UTHornsRawk  candidates.  https://t.co/nth63AgX5C   ...    “Democrats  \\\n",
       "0             0            0                        0   ...             0   \n",
       "1             0            0                        0   ...             0   \n",
       "2             0            0                        0   ...             0   \n",
       "3             0            0                        0   ...             0   \n",
       "4             0            0                        0   ...             0   \n",
       "\n",
       "   kinda  musulmanes,  https://t.co/CTBIkN50VE  coffee.  Garbage  metà,  \\\n",
       "0      0            0                        0        0        0      0   \n",
       "1      0            0                        0        0        0      0   \n",
       "2      0            0                        0        0        0      0   \n",
       "3      0            0                        0        0        0      0   \n",
       "4      0            0                        0        0        0      0   \n",
       "\n",
       "   Contributor  understand  Adams:  \n",
       "0            0           0       0  \n",
       "1            0           0       0  \n",
       "2            0           0       0  \n",
       "3            0           0       0  \n",
       "4            0           0       0  \n",
       "\n",
       "[5 rows x 7739 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the first few rows\n",
    "dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's all zeros! Let's see how bad it really is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9980055252650599"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of non zero entries\n",
    "n_non_zero = dtm.astype(bool).sum().sum()\n",
    "\n",
    "# Totoal number of entries\n",
    "tot = 1521 * 7739\n",
    "\n",
    "# Get the percentage of zeros\n",
    "1 - float(n_non_zero) / float(tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is really bad. 99.8% of our matrix is zeros. That means we generated a very big matrix that stores very little information. And most terms only occur once in one document, are therefore useless for comparative analysis. This problem leads us to the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before that, some resources for term document matrix generation:\n",
    "\n",
    "The R packages I mentioned above help you generate term document matrices automatically. In python I can recommend the [gensim](https://radimrehurek.com/gensim/) package. It gives you utilities to make document term matrices and fit sophisticated models to your data. However you have to pre-process your the text yourself. Good that you just learned how to do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sparsity Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected before, here is why we have such a sparse matrix. Words that mean the same thing are treated as completely independent dimensions. E.g.:\n",
    "\n",
    "- Capitalization: 'And', 'and', 'AND'\n",
    "- Punctuation: 'dog,', 'dog.', 'dog'\n",
    "- Grammatical inflections: 'walking' vs. 'walked' vs. 'walk'\n",
    "- Irregular grammatical forms: 'dive', 'dove'\n",
    "- Typos and other weirdness: 'nad', 'and', 'niiiceee', 'nice'\n",
    "- unique urls, twitter handles and hashtags\n",
    "- etc.\n",
    "\n",
    "This has several problematic consequences:\n",
    "\n",
    "- Memory requirements\n",
    "- Processing requirements\n",
    "- Analytical problems related to sparsity:\n",
    "\n",
    "Documents that could be considered similar can seem very distant for the computer. E.g. consider three documents:\n",
    "        1. 'The dog eats a cat'\n",
    "        2. 'the dogs eat many bananas'\n",
    "        3. 'I like coffee very much'\n",
    "\n",
    "The corresponding term document matrix would look like this:\n",
    "\n",
    "```\n",
    "    The dog eats a cat the dogs eat many bananas I like coffee very much\n",
    "1   1   1   1    1 1   0   0    0   0    0       0 0    0      0    0\n",
    "2   0   0   0    0 0   1   1    1   1    1       0 0    0      0    0\n",
    "3   0   0   0    0 0   0   0    0   0    0       1 1    1      1    1\n",
    "```\n",
    "\n",
    "Calculating the euclidian distance (or any other metric) between the vector for each word shows that they are all equidistant from each other. However, one could argue that documents `1` and `2` are closer to each other than they are to document `3`. We might want to see something like this:\n",
    "\n",
    "```\n",
    "    the dog eat a cat many bananas I like coffee very much\n",
    "1   1   1   1    1 1  0    0       0 0    0      0    0\n",
    "2   1   1   1    0 0  1    1       0 0    0      0    0\n",
    "3   0   0   0    0 0  0    0       1 1    1      1    1\n",
    "```\n",
    "\n",
    "Now you probably say 'of course, we have to make everything lowercase, remove all the punctuation and stem the tokens and remove all the stopwords'. These steps are often taken as a routine in text analysis applications. In most cases most of these standard pre-processing steps make absolute sense, but I think they should not be done mindlessly. Depending on the analysis some of these things that we remove here might be of value later. For example, capitalization of words might be important to differentiate between proper names and other words. The use of punctuation might be very informative when trying to attribute text to a specific person (don't we all know someone who uses way too many exclamation marks!!!!).\n",
    "\n",
    "Instead, I will discuss these steps as feature selection or dimensionality reduction techniques. Although they are normally separate steps in the analysis and there are statistical techniques to do them, I think it makes conceptually sense to treat them as the same. Consider using some form of latent factor analysis to retrieve a certian number of factors from the DTM and represent the documents in this reduced space. Although technically different, it is conceptually similar to converting tokens to lower case: We would expect that `Dog` and `dog` are highly correlated in the data because they mean basically the same thing. Therefore, we can represent the two variables as one, that we label `dog` and load with the sum of the old variables.\n",
    "\n",
    "There are many ways to deal with sparsity, but almost all of them involve reducing the size of the vocabulary, while maintaining as much relevant information as possible. Here is a list of things we can do:\n",
    "\n",
    "1. Convert all words to lowercase\n",
    "2. Remove all punctuation, numbers, special characters, etc.\n",
    "3. Remove very common words that contain little semantic information like: `and`, `or`, `the`, etc. also called *stopwords*\n",
    "4. Remove very infrequent words. Words that appear only once in the whole corpus are likely typos. Very infrequent words are also not very relevant for statistical analysis (but see above, depending on the analysis and outlier can be very informative).\n",
    "5. Stemming \n",
    "6. Lemmatization\n",
    "7. Use dimension reduction techniques such as PCA, Factor Analysis, Neural Networks, Topic Models, etc. to locate documents in a [semantic space](https://en.wikipedia.org/wiki/Word_embedding). \n",
    "8. Reduce vocabulary by inspecting the information content the words have for a supervise task, for example with $\\chi^2$-test\n",
    "    \n",
    "For this basic tutorial I will demonstrate just the basics (`1`-`7`) and completely ignore the statistical techniques. I hope to do a more advanced tutorial where I will cover these topics.\n",
    "\n",
    "Now lets do this. `1`, `2` and `3` can be easily done with regular expressions and standard string tools in every programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction through Text Preprocessing\n",
    "\n",
    "In this step we will convert everything to lower case and remove all non-letters. We will also remove so called stop words (words like `and`, `the`, `of` that don't contain much meaning but are very frequent). There are some special things in twitter data that we can also remove for now: \n",
    "- Many links/urls\n",
    "- Hashtags\n",
    "- Handles\n",
    "- Emojis\n",
    "\n",
    "They can be as informative as words, or not. They can be removed with regular expressions, see above. I will show how to remove them. The regular expressions I use probably miss a few. It's a lot of trial an error or googling to optimize precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First generate the regular expressions we compile them so they are quick to use\n",
    "# repeatedly in the loop below\n",
    "non_alpha = re.compile(r'[^A-Za-z #@]') # Everything that is not a letter or a space or @ or #\n",
    "excess_space = re.compile(r' +') # One or more spaces\n",
    "hasht = re.compile(r'#[^\\s$\\n]+') # hashtags\n",
    "hand = re.compile(r'@[^\\s$\\n]+') # handles\n",
    "url = re.compile(r'http\\S+') # urls\n",
    "\n",
    "# Load a stopword list from file. You can find those online\n",
    "stopwords = set(io.open('stopwords.txt', 'r').read().split('\\n'))\n",
    "\n",
    "# List to store the results\n",
    "clean_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    \n",
    "    # convert everything to lowercase\n",
    "    text = tweet.lower()\n",
    "    \n",
    "    # Remove handles and hashtags\n",
    "    text = hasht.sub(' ', text)\n",
    "    text = hand.sub(' ', text)\n",
    "    text = url.sub(' ', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = [w for w in text.split(' ') if w not in stopwords]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    # removce non-letters and excess space\n",
    "    text = non_alpha.sub(repl=' ', string=text)\n",
    "    text = excess_space.sub(repl=' ', string=text)\n",
    "    \n",
    "    clean_tweets.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what that does to an example tweet. The first line is the original tweet the second is the cleaned one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @donnabrazile: Amazing. https://t.co/SfxHD92gCJ\n",
      "rt amazing \n"
     ]
    }
   ],
   "source": [
    "print tweets[0]\n",
    "print clean_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much left. Now let's regenerate the DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I will write the DTM generation as a function, because we will re-use it later again and \n",
    "# we don't want to write everything three times. This is the same code as above, so I remove\n",
    "# all comments\n",
    "\n",
    "def generate_dtm(tweet_list):\n",
    "    bow_tweets = []\n",
    "    for tweet in clean_tweets:\n",
    "        tokens = tweet.split(' ')\n",
    "        counts = {}\n",
    "        for token in tokens:\n",
    "            if token in  counts:\n",
    "                counts[token] += 1\n",
    "            else:\n",
    "                counts[token] = 1\n",
    "        bow_tweets.append(counts)\n",
    "    vocabulary = set()\n",
    "    for tweet in bow_tweets:\n",
    "        vocabulary.update(tweet.keys())            \n",
    "    dtm = pd.DataFrame(columns = list(vocabulary))\n",
    "    i = 0\n",
    "    for tweet in bow_tweets:\n",
    "        vector = []\n",
    "        for token in dtm.columns:\n",
    "            if token in tweet:\n",
    "                vector.append(tweet[token])\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        dtm.loc[i] = vector\n",
    "        i += 1\n",
    "    return dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dtm = generate_dtm(tweet_list=clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1521, 3308)\n"
     ]
    }
   ],
   "source": [
    "print new_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reduced the number of words by a lot. From over 7000 to 3300! But we can do better. We still didn't solve the problem of gramatical inflections, plurals, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Stemming and lemmatization are two techniques to 'normalize' tokens. This is done to avoid differentiating between different grammatical forms of the same word. Consider the three examples: \n",
    "    - (walk, walking) \n",
    "    - (dive, dove) \n",
    "    - (doves, dove)\n",
    "    - (is, are)\n",
    "    \n",
    "The first is simple, the second is an irregular verb and the third is an animal. \n",
    "\n",
    "### Stemming\n",
    "\n",
    "Stemming algorithms are rule based and operate on the tokens itself. It returns the 'stem' of a word, i.e. without grammatical inflections etc. For the above example it would probably return something like:\n",
    "    - (walk, walk)\n",
    "    - (div, dov)\n",
    "    - (dov, dov)\n",
    "    - (is, are)\n",
    "It worked fine in the first place, but stemmers are not able to find the cannonical form (or lemma) of a word. Therefore it failed to figure out the last three cases.\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "Lemmatization, as the name suggests, is a group of algorithms that allow to find the lemma of a word. It often depends  on the context what the real lemma is (for example `the dove flies` or `I dove into the data`). The context of a word or the function of a word (verb, subject, object, etc.) can be automatically detected. This is called [part-of-speech tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging). Since it is important where in the sentence a word is located and what other words are around it, lemmatization works better if the algorithm is applied to the text in it's original form (not to text that has punctuation, upper case and stopwords removed for example). \n",
    "For the examles above, lemmatization would produce results like this:\n",
    "    - (walk, walk)\n",
    "    - (dive, dive)\n",
    "    - (dove, dove)\n",
    "    - (be, be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the difference between stemming and lemmatization on an example tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For stemming and lemmatization we have to rely on modules because it \n",
    "# would be to complex for this tutorial to write it from scratch\n",
    "from spacy.en import English # For lemmatization\n",
    "import Stemmer # PyStemmer module for stemming \n",
    "\n",
    "# INitialize the stemmer\n",
    "stemmer = Stemmer.Stemmer('english')\n",
    "\n",
    "# Initialize the spacy parser for lemmatization\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First the stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@tyreke977 Bernie has publically stated he doesn't endorse their acts. Trump, on the other hand, offers to pay legal fees? YUGE difference.\n",
      "\n",
      "\n",
      "@tyreke977 -> @tyreke977\n",
      "Bernie -> Berni\n",
      "has -> has\n",
      "publically -> public\n",
      "stated -> state\n",
      "he -> he\n",
      "doesn't -> doesn't\n",
      "endorse -> endors\n",
      "their -> their\n",
      "acts. -> acts.\n",
      "Trump, -> Trump,\n"
     ]
    }
   ],
   "source": [
    "text = tweets[262]\n",
    "\n",
    "print text\n",
    "print '\\n'\n",
    "tokens = text.split(' ')\n",
    "\n",
    "# This is some code to nicely print \n",
    "# the result and stop after 10 tokens\n",
    "for i, token in enumerate(tokens):\n",
    "    print '{} -> {}'.format(token, stemmer.stemWord(token))\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line shows the original tweet. Then below the first 10 words, on the left the original, after the arrow the stemmed token. It does some useful things, for example, `publically` becomes `public` and `stated` becomes `state`. However, we might not want to change `Bernie` to `Berni` and it didn't get that `doesn't` is the same as `does not`.\n",
    "\n",
    "Let's see what lemmatization does to this tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@tyreke977 -> @tyreke977\n",
      "Bernie -> bernie\n",
      "has -> have\n",
      "publically -> publically\n",
      "stated -> state\n",
      "he -> he\n",
      "does -> do\n",
      "n't -> not\n",
      "endorse -> endorse\n",
      "their -> their\n",
      "acts -> act\n",
      ". -> .\n",
      "Trump -> trump\n",
      ", -> ,\n",
      "on -> on\n",
      "the -> the\n"
     ]
    }
   ],
   "source": [
    "# Process the text with the spacy parser. \n",
    "parsed_text = parser(text)\n",
    "\n",
    "for i, token in enumerate(parsed_text):\n",
    "    print '{} -> {}'.format(token.orth_, token.lemma_)\n",
    "    if i == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better and a little bit like magic. `Bernie` stays `Bernie` it recognizes that `publically` is the adverb and that `doesn't` means `does not`. It also autmatically recogized that the `.` and `,` are not part of the words `Trump` and `act` (note that it realized that `'` belongs to `don't`). It also transformed `has` to its lemma `have`. If you want ot learn more about how it workes [this](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3359276/) paper has an overview of several algorithms. \n",
    "\n",
    "\n",
    "Note that, although, you might not see it on only one tweet, this is a lot slower than stemming, because there are some more complex operations happening under the hood. This might be a reason to use stemming if you have a large text collection and just need som rough normalization.\n",
    "\n",
    "Now let's make our final DTM. We redo all the steps above and add the lemmatization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First the cleaning. We do this again because we first want to lemmatize, when\n",
    "# we still have the intact tweets and then do the cleaning steps applied above\n",
    "\n",
    "clean_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "   \n",
    "    # Remove handles and hashtags\n",
    "    text = hasht.sub(' ', tweet)\n",
    "    text = hand.sub(' ', text)\n",
    "    text = url.sub(' ', text)\n",
    "    \n",
    "    # Lemmatize\n",
    "    parsed_text = parser(text)\n",
    "    text = ' '.join([t.lemma_ for t in parsed_text])\n",
    "    \n",
    "    # convert everything to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = [w for w in text.split(' ') if w not in stopwords]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    # removce non-letters and excess space\n",
    "    text = non_alpha.sub(repl=' ', string=text)\n",
    "    text = excess_space.sub(repl=' ', string=text)\n",
    "    \n",
    "    clean_tweets.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_dtm = generate_dtm(tweet_list=clean_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if this made a difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1521, 2805)\n"
     ]
    }
   ],
   "source": [
    "print final_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It didn't drastically change the number of words, but at least removed it by about 500. Note also that this reduction is probably very sensible, since lemmas that are the same are probably truly the same. That means we reduced the size of the matrix with little information loss (whereas with the more crude methods of regex removal we actually thre out a bunch of information).\n",
    "\n",
    "Let's take a look at our final vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'', u'child', u'bear', u'potencialmente', u'drumpf', u'protest',\n",
      "       u'asian', u'controversial', u'hate', u'increase',\n",
      "       ...\n",
      "       u'veil', u'gooooooo', u'baker', u'rule', u'cleve', u'determination',\n",
      "       u'yell', u'jivin', u'defend', u'simpson'],\n",
      "      dtype='object', length=2805)\n"
     ]
    }
   ],
   "source": [
    "print final_dtm.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a lot more sensible than what we saw on the first try. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>child</th>\n",
       "      <th>bear</th>\n",
       "      <th>potencialmente</th>\n",
       "      <th>drumpf</th>\n",
       "      <th>protest</th>\n",
       "      <th>asian</th>\n",
       "      <th>controversial</th>\n",
       "      <th>hate</th>\n",
       "      <th>increase</th>\n",
       "      <th>...</th>\n",
       "      <th>veil</th>\n",
       "      <th>gooooooo</th>\n",
       "      <th>baker</th>\n",
       "      <th>rule</th>\n",
       "      <th>cleve</th>\n",
       "      <th>determination</th>\n",
       "      <th>yell</th>\n",
       "      <th>jivin</th>\n",
       "      <th>defend</th>\n",
       "      <th>simpson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2805 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      child  bear  potencialmente  drumpf  protest  asian  controversial  \\\n",
       "0  1      0     0               0       0        0      0              0   \n",
       "1  0      0     0               0       0        0      0              0   \n",
       "2  1      0     0               0       0        0      0              0   \n",
       "3  1      0     0               0       0        1      0              0   \n",
       "4  0      0     0               0       0        0      0              0   \n",
       "\n",
       "   hate  increase   ...     veil  gooooooo  baker  rule  cleve  determination  \\\n",
       "0     0         0   ...        0         0      0     0      0              0   \n",
       "1     0         0   ...        0         0      0     0      0              0   \n",
       "2     0         0   ...        0         0      0     0      0              0   \n",
       "3     0         0   ...        0         0      0     0      0              0   \n",
       "4     0         0   ...        0         0      0     0      0              0   \n",
       "\n",
       "   yell  jivin  defend  simpson  \n",
       "0     0      0       0        0  \n",
       "1     0      0       0        0  \n",
       "2     0      0       0        0  \n",
       "3     0      0       0        0  \n",
       "4     0      0       0        0  \n",
       "\n",
       "[5 rows x 2805 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this matrix can be used for a variety of analyses. Ranging from simple word counts to sophisticated statistical models.\n",
    "\n",
    "Even after all these tedious steps, DTMs in most applications will be still very sparse. In a future tutorial I will talk in more detail about this. If you have a big corpus, consider generating a sparse matrix (only non-zero entries are stored) instead of a dense matrix. Many statistical analyses can be done on matrices in sparse format, so you save a lot of time and RAM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
